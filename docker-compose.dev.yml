
services:

  # BATCH DATA PRODUCER
  batch-producer:
    build:
      context: ./producer
      dockerfile: Dockerfile
    container_name: ratp-batch-producer
    environment:
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin123
      - PRODUCER_MODE=batch
      - BATCH_SCHEDULE=0 */6 * * *
      - LOG_LEVEL=INFO
    volumes:
      - ./producer:/app
      - ./data:/app/data
      - ml-models:/app/models
      - app-logs:/app/logs
    env_file: .env
    networks:
      - ratp-network
    restart: unless-stopped
    profiles:
      - batch


  # STREAMING PRODUCER (MÉTÉO)
  streaming-producer:
    build:
      context: ./producer
      dockerfile: Dockerfile
    container_name: ratp-streaming-producer
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:29092,kafka-2:29093
      - PRODUCER_MODE=streaming
      - STATION=auber
      - SEND_INTERVAL=60
      - LOG_LEVEL=INFO
    volumes:
      - ./producer:/app
      - ./data:/app/data
    env_file: .env
    networks:
      - ratp-network
    restart: unless-stopped
    depends_on:
      - kafka-1
      - kafka-2
    profiles:
      - streaming


  # SPARK STREAMING JOB (MÉTÉO )
  spark-streaming:
    build:
      context: ./spark-jobs
      dockerfile: Dockerfile
    container_name: ratp-spark-streaming
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:29092,kafka-2:29093
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin123
      - CHECKPOINT_LOCATION=/tmp/checkpoints
      - STREAMING_TOPICS=meteo-data
    env_file: .env
    volumes:
      - ./spark-jobs:/app
      - spark-checkpoints:/tmp/checkpoints
    networks:
      - ratp-network
    restart: unless-stopped
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
    profiles:
      - streaming


  # SPARK ETL JOB (BATCH + STREAMING FUSION)
  spark-etl:
    build:
      context: ./spark-jobs
      dockerfile: Dockerfile
    container_name: ratp-spark-etl
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin123
      - ETL_SCHEDULE=0 */12 * * *
      - ETL_MODE=hybrid
    volumes:
      - ./spark-jobs:/app
      - ./data:/app/data
    env_file: .env
    networks:
      - ratp-network
    restart: unless-stopped
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
    profiles:
      - etl


  # STREAMLIT DASHBOARD
  streamlit-app:
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    container_name: ratp-dashboard
    ports:
      - "8501:8501"
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:29092,kafka-2:29093
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin123
      - STATION=auber
      - MODEL_PATH=/app/models/pm10_predictor.pkl
      - DATA_SOURCES=batch,streaming
    volumes:
      - ./dashboard:/app
      - ./data:/app/data
    networks:
      - ratp-network
    restart: unless-stopped
    depends_on:
      - minio
    profiles:
      - dashboard


# VOLUMES
volumes:
  # Spark fault tolerance (isolation)
  spark-checkpoints:
    driver: local
  ml-models:
    driver: local
  app-logs:
    driver: local


# NETWORK
networks:
  ratp-network:
    driver: bridge
    name: ratp-network